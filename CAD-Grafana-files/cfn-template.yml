---
AWSTemplateFormatVersion: '2010-09-09'
Description: Retrieves AWS Cost Explorer Cost Anomalies details
Resources:
  awscostanomolybucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'aws-cost-anomalies-${AWS::AccountId}-${AWS::Region}'
      AccessControl: Private
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
  costAthenaDatabase:
    Type: AWS::Glue::Database
    Properties:
      DatabaseInput:
        Name: 'aws-cost-anomolies'
      CatalogId: !Ref AWS::AccountId
  costanomoliesGlueCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: aws-cost-anomolies
      Role: !Ref 'costanomoliesGlueCrawlerRole'
      DatabaseName: !Ref costAthenaDatabase
      Targets:
        S3Targets:
          - Path: !Sub 's3://${awscostanomolybucket}/aws-cost-anomolies/'
  costanomoliesGlueCrawlerRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - glue.amazonaws.com
            Action:
              - sts:AssumeRole
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Path: /
      Policies:
        - PolicyName: CE-GlueCrawlerRolePolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:ListBucket
                  - s3:GetObject
                Resource:
                  - !Sub 'arn:${AWS::Partition}:s3:::${awscostanomolybucket}*'
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource:
                  - !Sub 'arn:${AWS::Partition}:s3:::${awscostanomolybucket}*'
  costanomoliesLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub
        - aws-cost-anomolies-${Id}
        - Id: !Select
            - 0
            - !Split
              - '-'
              - !Ref 'AWS::StackName'
      Handler: index.lambda_handler
      Runtime: python3.10
      MemorySize: 2688
      Timeout: 60
      Role: !GetAtt 'costanomoliesLambdaFunctionRole.Arn'
      Environment:
        Variables:
          S3_BUCKET: !Ref 'awscostanomolybucket'
          CRAWLER_NAME: !Ref 'costanomoliesGlueCrawler'
          PREFIX: aws-cost-anomolies
          ManagementAccountId: !Sub '${AWS::AccountId}'
      Code:
        ZipFile: |
          import os
          import json
          import logging
          from datetime import date, timedelta, datetime
          import boto3
          from boto3.s3.transfer import S3Transfer

          BUCKET = os.environ['S3_BUCKET']
          PREFIX = os.environ['PREFIX']
          CRAWLER_NAME = os.environ['CRAWLER_NAME']
          MANAGEMENT_ACCOUNT_ID = os.environ['ManagementAccountId']

          def start_crawler():
              glue = boto3.client('glue')
              try:
                  glue.start_crawler(Name=CRAWLER_NAME)
              except Exception as exc:
                  logging.warning(exc)

          def store_data_to_s3(flattened_data, path):
              today = date.today()
              year = today.year
              month = today.strftime('%m')
              day = today.day
              local_file = '/tmp/tmp.json'
              with open(local_file, 'w') as f:
                  #json.dump(flattened_data, f, default=str)
                  f.write('\n'.join([json.dumps(result) for result in flattened_data]))

              if os.path.getsize(local_file) == 0:
                  print(f"No data in file for {path}")
                  return

              s3client = boto3.client('s3')
              key = today.strftime(f"{path}/year={year}/month={month}/day={day}/{year}-{month}-{day}.json")
              print(f"Uploading file {local_file} to {BUCKET}/{key}")
              S3Transfer(s3client).upload_file(local_file, BUCKET, key, extra_args={'ACL': 'bucket-owner-full-control'})
              print('file upload successful')

          def get_ce_costanomaly(ce, start_date, end_date):
              results = []
              next_token = None
              while True:
                  params = dict(
                      DateInterval={
                        'StartDate': str(start_date),
                        'EndDate': str(end_date)
                      },
                      MaxResults=100,
                  )
                  if next_token:
                      params['NextPageToken'] = next_token
                  response = ce.get_anomalies(**params)
                  results += response['Anomalies']
                  if 'NextPageToken' in response:
                      next_token = response['NextPageToken']
                  else:
                      break
              return results

          def flatten_results(results):
              flattened_results = []
              for anomaly in results:
                  flattened_anomaly = {
                      'AnomalyId': anomaly['AnomalyId'],
                      'AnomalyStartDate': anomaly['AnomalyStartDate'],
                      'AnomalyEndDate': anomaly['AnomalyEndDate'],
                      'DimensionValue': anomaly['DimensionValue'],
                      'MaxImpact': anomaly['Impact']['MaxImpact'],
                      'TotalActualSpend': anomaly['Impact']['TotalActualSpend'],
                      'TotalExpectedSpend': anomaly['Impact']['TotalExpectedSpend'],
                      'TotalImpact': anomaly['Impact']['TotalImpact'],
                      'TotalImpactpercentage': anomaly['Impact'].get('TotalImpactPercentage', 0),
                      'MonitorArn': anomaly['MonitorArn'],
                      'LinkedAccount': anomaly['RootCauses'][0].get('LinkedAccount'),
                      'LinkedAccountName': anomaly['RootCauses'][0].get('LinkedAccountName'),
                      'Region': anomaly['RootCauses'][0].get('Region'),
                      'Service': anomaly['RootCauses'][0].get('Service'),
                      'UsageType': anomaly['RootCauses'][0].get('UsageType')
                  }
                  flattened_results.append(flattened_anomaly)
              return flattened_results

          def calculate_dates(s3_path):
              end_date = datetime.now().date()
              start_date = datetime.now().date() - timedelta(days=90) #Cost anomalies are available for last 90days

              # Check the create time of objects in the S3 bucket
              paginator = boto3.client('s3').get_paginator('list_objects_v2')
              contents = sum( [page.get('Contents', []) for page in paginator.paginate(Bucket=BUCKET, Prefix=s3_path)], [])
              last_modified_date = get_last_modified_date(contents)
              if last_modified_date and last_modified_date >= start_date:
                  start_date = last_modified_date
              return start_date, end_date

          def get_last_modified_date(contents):
              last_modified_dates = [obj['LastModified'].date() for obj in contents]
              last_modified_dates_within_90_days = [date for date in last_modified_dates if date >= datetime.now().date() - timedelta(days=90)]
              if last_modified_dates_within_90_days:
                  return max(last_modified_dates_within_90_days)
              return None

          def lambda_handler(event, context):
              logger = logging.getLogger()
              sts = boto3.client('sts')
              start_date, end_date = calculate_dates(s3_path=f'{PREFIX}/cost-anomaly-data/')
              print(f'start_date={start_date}, end_date={end_date}')
              total_count = 0
              ce = boto3.client('ce')
              data = get_ce_costanomaly(ce, start_date, end_date)
              flattened_data = flatten_results(data)
              total_count += len(flattened_data)
              store_data_to_s3(flattened_data, f'{PREFIX}/cost-anomaly-data/management_id={MANAGEMENT_ACCOUNT_ID}')
              if total_count:
                  start_crawler()
              return "Successful"
  LambdaLogGroup:
        Type: AWS::Logs::LogGroup
        Properties:
          LogGroupName: !Sub "/aws/lambda/${costanomoliesLambdaFunction}"
          RetentionInDays: 7
  costanomoliesLambdaFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: AWS-Cost-Anomolies-Execute-Lambda
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
  S3BucketAccessPolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: Cost-Anomaly-S3BucketAccessPolicy
      Roles:
        - !Ref 'costanomoliesLambdaFunctionRole'
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - s3:ListBucket
              - s3:PutObject
              - s3:GetObject
            Resource:
              - !Sub 'arn:${AWS::Partition}:s3:::${awscostanomolybucket}'
              - !Sub 'arn:${AWS::Partition}:s3:::${awscostanomolybucket}/*'
          - Effect: Allow
            Action:
              - logs:CreateLogGroup
              - logs:CreateLogStream
              - logs:PutLogEvents
              - logs:DescribeLogStreams
            Resource:
              - !Sub 'arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws-cost-explorer-cost-anomaaly-recommendations-function*'
  costanomoliesGlueCrawlerPolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: Cost-Anomaly-GlueCrawlerPolicy
      Roles:
        - !Ref 'costanomoliesLambdaFunctionRole'
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - glue:StartCrawler
            Resource:
              - !Sub 'arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:crawler/${costanomoliesGlueCrawler}'
  costanomoliesCloudWatchLogsPolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: Cost-Anomaly-CloudWatchLogsPolicy
      Roles:
        - !Ref 'costanomoliesLambdaFunctionRole'
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - logs:CreateLogGroup
              - logs:CreateLogStream
              - logs:PutLogEvents
            Resource: arn:aws:logs:*:*:*
  CostExplorerPolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: CE-A-CostExplorerPolicy
      Roles:
        - !Ref 'costanomoliesLambdaFunctionRole'
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - ce:GetAnomalies
            Resource: '*'
  costanomoliesAthenaView:
    Type: AWS::Athena::NamedQuery
    Properties:
      Name: AWS-Cost-Anomaly
      Database: aws-cost-anomalies-db
      Description: Provides a summary view of the AWS Cost Anomalies
      QueryString: |
        CREATE OR REPLACE VIEW "ca_summary_view" AS  SELECT DISTINCT
          "year"
        , "month" , anomalyid , dimensionvalue , management_id "ManagementAccountId", CAST("from_iso8601_timestamp"("anomalystartdate") AS date) anomaly_start_date , CAST("from_iso8601_timestamp"("anomalyenddate") AS date) anomalylastupdatedate , CAST(maximpact AS int) maximpact , CAST(totalactualspend AS int) totalactualspend , CAST(totalexpectedspend AS int) totalexpectedspend , CAST(totalimpact AS int) tota_impact , CAST(totalimpactpercentage AS int) totalimpactpercentage , (CASE WHEN ((anomalyenddate IS NULL) OR (CAST("substring"(anomalyenddate, 1, 10) AS date) < (current_date - INTERVAL  '2' DAY))) THEN 'Closed' ELSE 'Open' END) Status , EXTRACT(DAY FROM (CAST("from_iso8601_timestamp"("anomalyenddate") AS date) - CAST("from_iso8601_timestamp"("anomalystartdate") AS date))) duration , monitorarn , 
            region , service , linkedaccount , linkedaccountname  , usagetype FROM
          "AwsDataCatalog"."aws-cost-anomolies"."aws_cost_anomolies"
        WHERE ("date_parse"("concat"("year", "month", "day"), '%Y%m%d') >= (current_timestamp - INTERVAL  '1' MONTH)) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,19,20
  CloudWatchTrigger:
    Type: AWS::Events::Rule
    Properties:
      Description: Notification Event for lambda trigger
      Name: !Sub "cost-anomaly-Scheduler"
      ScheduleExpression: rate(1 day)
      State: ENABLED
      Targets:
        - Arn: !GetAtt costanomoliesLambdaFunction.Arn
          Id: TriggerLambda
  EventPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt costanomoliesLambdaFunction.Arn
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceAccount: !Ref "AWS::AccountId"
      SourceArn: !GetAtt CloudWatchTrigger.Arn
Outputs:
  CostExplorercostanomoliesLambdaRoleARN:
    Description: Role for Lambda execution of AWS Cost Explorer cost anomolies extraction.
    Value: !GetAtt 'costanomoliesLambdaFunctionRole.Arn'
  LambdaCostExplorerRightsizingARN:
    Description: Lambda function ARN for retrieving AWS Cost Explorer cost anomolies extraction
    Value: !GetAtt 'costanomoliesLambdaFunction.Arn'
  GlueCrawler:
    Description: AWS Glue Crawler for crawling AWS cost anomolies from the S3 bucket
    Value: !Ref 'costanomoliesGlueCrawler'
  S3Bucket:
    Description: Name of S3 Bucket which will store the AWS Cost Explorer cost anomolies recommendations
    Value: !Ref 'awscostanomolybucket'
